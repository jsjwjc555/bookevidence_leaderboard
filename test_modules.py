#!/usr/bin/env python3
"""
æ¨¡å—æµ‹è¯•è„šæœ¬
æµ‹è¯•å„ä¸ªåŠŸèƒ½æ¨¡å—æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import sys
import os
import json
import traceback
from datetime import datetime

BASE_DIR = os.path.abspath(os.path.dirname(__file__))

def resolve_data_dir(default_path: str) -> str:
    raw_path = os.environ.get("WEB_LEADERBOARD_DATA_DIR_V4") or os.environ.get("WEB_LEADERBOARD_DATA_DIR")
    return os.path.abspath(raw_path or default_path)

def test_imports():
    """æµ‹è¯•æ¨¡å—å¯¼å…¥"""
    print("ğŸ§ª æµ‹è¯•æ¨¡å—å¯¼å…¥...")
    
    try:
        import streamlit as st
        print("âœ… Streamlit å¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"âŒ Streamlit å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    try:
        import pandas as pd
        print("âœ… Pandas å¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"âŒ Pandas å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    try:
        import plotly.express as px
        print("âœ… Plotly å¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"âŒ Plotly å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    try:
        from data_manager import DatasetManager, LeaderboardManager
        print("âœ… æ•°æ®ç®¡ç†æ¨¡å—å¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"âŒ æ•°æ®ç®¡ç†æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    try:
        from model_api import ModelAPIFactory, AttributionMetrics
        print("âœ… æ¨¡å‹APIæ¨¡å—å¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"âŒ æ¨¡å‹APIæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    return True

def test_data_manager():
    """æµ‹è¯•æ•°æ®ç®¡ç†å™¨"""
    print("\nğŸ§ª æµ‹è¯•æ•°æ®ç®¡ç†å™¨...")
    
    try:
        from data_manager import DatasetManager, LeaderboardManager
        
        # æµ‹è¯•æ•°æ®é›†ç®¡ç†å™¨
        data_dir = resolve_data_dir("/Users/chengyihao/Documents/vscode-python/web_leaderboard/BookEvidenceQA_v4")
        
        if os.path.exists(data_dir):
            dataset_manager = DatasetManager(data_dir)
            domains = dataset_manager.get_domains()
            print(f"âœ… æ•°æ®é›†ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸï¼Œæ‰¾åˆ° {len(domains)} ä¸ªé¢†åŸŸ")
            
            if domains:
                test_domain = domains[0]
                sample_ids = dataset_manager.get_sample_ids(test_domain)
                print(f"âœ… é¢†åŸŸ {test_domain} åŒ…å« {len(sample_ids)} ä¸ªæ ·æœ¬")
                
                if sample_ids:
                    test_sample = dataset_manager.get_sample(test_domain, sample_ids[0])
                    if test_sample:
                        print("âœ… æ ·æœ¬æ•°æ®è·å–æˆåŠŸ")
                    else:
                        print("âŒ æ ·æœ¬æ•°æ®è·å–å¤±è´¥")
        else:
            print(f"âš ï¸  æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        
        # æµ‹è¯•æ’è¡Œæ¦œç®¡ç†å™¨
        leaderboard_manager = LeaderboardManager()
        print("âœ… æ’è¡Œæ¦œç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•æ•°æ®åº“æ“ä½œ
        test_result = {
            "model_name": "TestModel",
            "model_type": "Test API",
            "Citation Precision": 0.85,
            "Citation Recall": 0.78,
            "F1 Score": 0.81,
            "Answer Similarity": 0.88,
            "test_domains": ["test"],
            "sample_count": 10,
            "config": {"temperature": 0.7}
        }
        
        eval_id = leaderboard_manager.save_evaluation_result(test_result)
        print(f"âœ… æµ‹è¯•è¯„æµ‹ç»“æœä¿å­˜æˆåŠŸï¼ŒID: {eval_id}")
        
        # æ¸…ç†æµ‹è¯•æ•°æ®
        leaderboard_manager.delete_evaluation(eval_id)
        print("âœ… æµ‹è¯•æ•°æ®æ¸…ç†å®Œæˆ")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®ç®¡ç†å™¨æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_model_api():
    """æµ‹è¯•æ¨¡å‹APIæ¨¡å—"""
    print("\nğŸ§ª æµ‹è¯•æ¨¡å‹APIæ¨¡å—...")
    
    try:
        from model_api import AttributionMetrics
        
        # æµ‹è¯•è¯„æµ‹æŒ‡æ ‡è®¡ç®—
        predicted_citations = ["å¼•ç”¨1", "å¼•ç”¨2", "å¼•ç”¨3"]
        reference_citations = ["å¼•ç”¨1", "å¼•ç”¨4", "å¼•ç”¨2"]
        
        precision = AttributionMetrics.calculate_citation_precision(
            predicted_citations, reference_citations
        )
        recall = AttributionMetrics.calculate_citation_recall(
            predicted_citations, reference_citations
        )
        f1 = AttributionMetrics.calculate_f1_score(precision, recall)
        
        print(f"âœ… æŒ‡æ ‡è®¡ç®—æˆåŠŸ: ç²¾åº¦={precision:.3f}, å¬å›={recall:.3f}, F1={f1:.3f}")
        
        # æµ‹è¯•ç­”æ¡ˆç›¸ä¼¼åº¦è®¡ç®—
        similarity = AttributionMetrics.calculate_answer_similarity(
            "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•ç­”æ¡ˆ", "è¿™æ˜¯æµ‹è¯•ç­”æ¡ˆå†…å®¹"
        )
        print(f"âœ… ç›¸ä¼¼åº¦è®¡ç®—æˆåŠŸ: {similarity:.3f}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹APIæ¨¡å—æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_database():
    """æµ‹è¯•æ•°æ®åº“è¿æ¥"""
    print("\nğŸ§ª æµ‹è¯•æ•°æ®åº“è¿æ¥...")
    
    try:
        import sqlite3
        
        # æµ‹è¯•SQLiteè¿æ¥
        conn = sqlite3.connect(":memory:")
        cursor = conn.cursor()
        
        # åˆ›å»ºæµ‹è¯•è¡¨
        cursor.execute('''
            CREATE TABLE test_table (
                id INTEGER PRIMARY KEY,
                name TEXT,
                value REAL
            )
        ''')
        
        # æ’å…¥æµ‹è¯•æ•°æ®
        cursor.execute("INSERT INTO test_table (name, value) VALUES (?, ?)", 
                      ("test", 1.23))
        
        # æŸ¥è¯¢æ•°æ®
        cursor.execute("SELECT * FROM test_table")
        result = cursor.fetchone()
        
        conn.close()
        
        if result:
            print("âœ… æ•°æ®åº“è¿æ¥å’Œæ“ä½œæ­£å¸¸")
            return True
        else:
            print("âŒ æ•°æ®åº“æ“ä½œå¤±è´¥")
            return False
            
    except Exception as e:
        print(f"âŒ æ•°æ®åº“æµ‹è¯•å¤±è´¥: {e}")
        return False

def create_sample_data():
    """åˆ›å»ºç¤ºä¾‹æ•°æ®ç”¨äºæµ‹è¯•"""
    print("\nğŸ§ª åˆ›å»ºç¤ºä¾‹æ•°æ®...")
    
    sample_data = {
        "test_sample_1": {
            "question": "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•é—®é¢˜ï¼Ÿ",
            "answer": "è¿™æ˜¯æµ‹è¯•é—®é¢˜çš„å‚è€ƒç­”æ¡ˆã€‚",
            "sentence_level_citation": [
                {
                    "sentence": "è¿™æ˜¯ç¬¬ä¸€ä¸ªå¥å­çš„å›ç­”ã€‚",
                    "citations": {
                        "anchor_text": ["ç›¸å…³å¼•ç”¨æ–‡æœ¬1"],
                        "prefix_text": ["è¿™æ˜¯å¼•ç”¨çš„ä¸Šä¸‹æ–‡"]
                    }
                },
                {
                    "sentence": "è¿™æ˜¯ç¬¬äºŒä¸ªå¥å­çš„å›ç­”ã€‚",
                    "citations": {
                        "anchor_text": ["ç›¸å…³å¼•ç”¨æ–‡æœ¬2", "ç›¸å…³å¼•ç”¨æ–‡æœ¬3"],
                        "prefix_text": ["ä¸Šä¸‹æ–‡1", "ä¸Šä¸‹æ–‡2"]
                    }
                }
            ]
        }
    }
    
    # ä¿å­˜ç¤ºä¾‹æ•°æ®
    test_file = os.path.abspath(os.path.join(BASE_DIR, "test_data.json"))
    try:
        with open(test_file, 'w', encoding='utf-8') as f:
            json.dump(sample_data, f, ensure_ascii=False, indent=2)
        print(f"âœ… ç¤ºä¾‹æ•°æ®å·²ä¿å­˜åˆ° {test_file}")
        
        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        os.remove(test_file)
        print("âœ… æµ‹è¯•æ–‡ä»¶æ¸…ç†å®Œæˆ")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç¤ºä¾‹æ•°æ®åˆ›å»ºå¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§  å¤§æ¨¡å‹å½’å› åˆ†æå¹³å° - æ¨¡å—æµ‹è¯•")
    print("=" * 50)
    print(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Pythonç‰ˆæœ¬: {sys.version}")
    print()
    
    tests = [
        ("æ¨¡å—å¯¼å…¥", test_imports),
        ("æ•°æ®ç®¡ç†å™¨", test_data_manager),
        ("æ¨¡å‹APIæ¨¡å—", test_model_api),
        ("æ•°æ®åº“è¿æ¥", test_database),
        ("ç¤ºä¾‹æ•°æ®", create_sample_data)
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        try:
            if test_func():
                passed += 1
                print(f"âœ… {test_name} æµ‹è¯•é€šè¿‡")
            else:
                print(f"âŒ {test_name} æµ‹è¯•å¤±è´¥")
        except Exception as e:
            print(f"âŒ {test_name} æµ‹è¯•å¼‚å¸¸: {e}")
        print("-" * 30)
    
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿå¯ä»¥æ­£å¸¸è¿è¡Œã€‚")
        print("\nğŸš€ å¯åŠ¨åº”ç”¨å‘½ä»¤:")
        print("   python enhanced_app.py")
        print("   æˆ–è€…")
        print("   streamlit run enhanced_app.py")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³æ¨¡å—ã€‚")
        print("\nğŸ”§ æ•…éšœæ’é™¤å»ºè®®:")
        print("1. æ£€æŸ¥ä¾èµ–åŒ…æ˜¯å¦æ­£ç¡®å®‰è£…: pip install -r requirements.txt")
        print("2. æ£€æŸ¥æ•°æ®é›†ç›®å½•æ˜¯å¦å­˜åœ¨ä¸”åŒ…å«æ­£ç¡®æ ¼å¼çš„æ–‡ä»¶")
        print("3. æ£€æŸ¥Pythonç‰ˆæœ¬æ˜¯å¦æ»¡è¶³è¦æ±‚ (3.8+)")

if __name__ == "__main__":
    main()
