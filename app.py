"""
å¤§æ¨¡å‹å½’å› åˆ†æå±•ç¤ºå¹³å°
ä½œè€…ï¼šAIåŠ©æ‰‹
åŠŸèƒ½ï¼š
1. æ¨¡å‹å½’å› æ•ˆæœå±•ç¤º
2. è‡ªå®šä¹‰æ¨¡å‹è¯„æµ‹
3. Leaderboardå±•ç¤º
4. æ•°æ®é›†æµè§ˆå’Œæµ‹è¯•
"""

import streamlit as st
import pandas as pd
import json
import plotly.express as px
import plotly.graph_objects as go
from typing import Dict, List, Any
import os
import sys
import random
from datetime import datetime

BASE_DIR = os.path.abspath(os.path.dirname(__file__))

def resolve_data_dir(default_path: str, env_var: str) -> str:
    raw_path = os.environ.get(env_var) or os.environ.get("WEB_LEADERBOARD_DATA_DIR")
    return os.path.abspath(raw_path or default_path)

def resolve_article_dir(default_path: str) -> str:
    raw_path = os.environ.get("WEB_LEADERBOARD_ARTICLE_DIR")
    return os.path.abspath(raw_path or default_path)

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥ç°æœ‰æ¨¡å—
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥Trust-Scoreç®¡ç†å™¨
try:
    from trust_score_manager import TrustScoreManager
except ImportError:
    # å¦‚æœåœ¨web_leaderboardç›®å½•ä¸‹è¿è¡Œ
    try:
        from web_leaderboard.trust_score_manager import TrustScoreManager
    except ImportError:
        TrustScoreManager = None

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="å¤§æ¨¡å‹å½’å› åˆ†æå¹³å°",
    page_icon="ğŸ§ ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šä¹‰CSSæ ·å¼
st.markdown("""
<style>
.main-header {
    font-size: 2.5rem;
    font-weight: bold;
    color: #1f77b4;
    text-align: center;
    margin-bottom: 2rem;
}

.metric-card {
    background-color: #f0f2f6;
    padding: 1rem;
    border-radius: 0.5rem;
    border-left: 4px solid #1f77b4;
}

.citation-box {
    background-color: #fff3cd;
    border: 1px solid #ffeaa7;
    border-radius: 0.25rem;
    padding: 0.75rem;
    margin: 0.5rem 0;
}

.evidence-box {
    background-color: #e8f5e8;
    border: 1px solid #28a745;
    border-radius: 0.25rem;
    padding: 0.75rem;
    margin: 0.5rem 0;
}

</style>
""", unsafe_allow_html=True)

class ArticleLoader:
    """æ–‡ç« åŠ è½½å™¨"""
    
    def __init__(self, article_dir: str):
        self.article_dir = article_dir
        self.articles = {}
        self._load_articles()
    
    def _load_articles(self):
        """åŠ è½½æ‰€æœ‰æ–‡ç« æ•°æ®"""
        if not os.path.exists(self.article_dir):
            print(f"æ–‡ç« ç›®å½•ä¸å­˜åœ¨: {self.article_dir}")
            return
            
        # è‡ªåŠ¨æ‰«ææ‰€æœ‰ *_article_hard.json æ–‡ä»¶
        for filename in os.listdir(self.article_dir):
            if filename.endswith('_article_hard.json'):
                file_path = os.path.join(self.article_dir, filename)
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    domain = filename.replace('_article_hard.json', '')
                    self.articles[domain] = data
                    print(f"æˆåŠŸåŠ è½½ {domain} æ–‡ç« æ•°æ®")
                except Exception as e:
                    print(f"åŠ è½½æ–‡ç« æ–‡ä»¶ {filename} å¤±è´¥: {str(e)}")
    
    def get_article(self, source_id: str) -> str:
        """æ ¹æ®source_idè·å–æ–‡ç« å†…å®¹"""
        for domain, articles in self.articles.items():
            if source_id in articles:
                return articles[source_id]
        return ""
    
    def get_article_stats(self, source_id: str) -> Dict[str, Any]:
        """è·å–æ–‡ç« ç»Ÿè®¡ä¿¡æ¯"""
        article_content = self.get_article(source_id)
        if not article_content:
            return {}
        
        # æå–æ ‡é¢˜ï¼ˆå‡è®¾ç¬¬ä¸€è¡Œæˆ–å‰å‡ è¡ŒåŒ…å«æ ‡é¢˜ï¼‰
        lines = article_content.split('\n')
        title = ""
        for line in lines[:10]:  # æ£€æŸ¥å‰10è¡Œ
            if line.strip() and ('**' in line or '#' in line or line.isupper()):
                title = line.strip().replace('**', '').replace('#', '').strip()
                break
        
        if not title:
            title = lines[0][:100] + "..." if lines else "æœªçŸ¥æ ‡é¢˜"
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        word_count = len(article_content.split())
        char_count = len(article_content)
        line_count = len([line for line in lines if line.strip()])
        
        # åŸºäºsource_idè¯†åˆ«ä¸»é¢˜
        topic_mapping = {
            'agriculture': 'å†œä¸š',
            'art': 'è‰ºæœ¯', 
            'history': 'å†å²',
            'technology': 'æŠ€æœ¯',
            'psychology': 'å¿ƒç†å­¦',
            'politics': 'æ”¿æ²»',
            'physics': 'ç‰©ç†',
            'philosophy': 'å“²å­¦',
            'music': 'éŸ³ä¹',
            'mix': 'ç»¼åˆ',
            'mathematics': 'æ•°å­¦',
            'literature': 'æ–‡å­¦',
            'legal': 'æ³•å¾‹',
            'health': 'å¥åº·',
            'fin': 'é‡‘è',
            'fiction': 'å°è¯´',
            'cs': 'è®¡ç®—æœºç§‘å­¦',
            'cooking': 'çƒ¹é¥ª',
            'biology': 'ç”Ÿç‰©å­¦',
            'biography': 'ä¼ è®°'
        }
        
        # ä»source_idæå–é¢†åŸŸå‰ç¼€
        domain_prefix = None
        for domain in topic_mapping.keys():
            if source_id.startswith(f'{domain}_'):
                domain_prefix = domain
                break
        
        if domain_prefix and domain_prefix in topic_mapping:
            topic = topic_mapping[domain_prefix]
        else:
            topic = "å…¶ä»–"
        
        return {
            'title': title,
            'topic': topic,
            'word_count': word_count,
            'char_count': char_count,
            'line_count': line_count
        }

class DatasetLoader:
    """æ•°æ®é›†åŠ è½½å™¨"""
    
    def __init__(self, data_dir: str):
        self.data_dir = data_dir
        self.datasets = {}
        self._load_datasets()
    
    def _load_datasets(self):
        """åŠ è½½æ‰€æœ‰æ•°æ®é›†"""
        if not os.path.exists(self.data_dir):
            st.error(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.data_dir}")
            return
            
        # è‡ªåŠ¨æ‰«ææ‰€æœ‰ *_qao_v3.json æ–‡ä»¶
        for filename in os.listdir(self.data_dir):
            if filename.endswith('_qao_v3.json'):
                file_path = os.path.join(self.data_dir, filename)
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    domain = filename.replace('_qao_v3.json', '')
                    self.datasets[domain] = data
                    print(f"æˆåŠŸåŠ è½½ {domain} æ•°æ®é›†: {len(data)} ä¸ªæ ·æœ¬")
                except Exception as e:
                    st.error(f"åŠ è½½ {filename} å¤±è´¥: {str(e)}")
    
    def get_domains(self) -> List[str]:
        """è·å–æ‰€æœ‰é¢†åŸŸ"""
        return list(self.datasets.keys())
    
    def get_sample_ids(self, domain: str) -> List[str]:
        """è·å–æŒ‡å®šé¢†åŸŸçš„æ ·æœ¬ID"""
        if domain in self.datasets:
            return list(self.datasets[domain].keys())
        return []
    
    def get_sample(self, domain: str, sample_id: str) -> Dict[str, Any]:
        """è·å–æŒ‡å®šæ ·æœ¬"""
        if domain in self.datasets and sample_id in self.datasets[domain]:
            return self.datasets[domain][sample_id]
        return {}

class AttributionEvaluator:
    """å½’å› è¯„ä¼°å™¨"""
    
    @staticmethod
    def calculate_citation_precision(model_answer: List[Dict]) -> float:
        """è®¡ç®—å¼•ç”¨ç²¾åº¦"""
        total_citations = 0
        valid_citations = 0
        
        for sentence in model_answer:
            citations = sentence.get('citations', {})
            anchor_texts = citations.get('anchor_text', [])
            total_citations += len(anchor_texts)
            
            # ç®€å•çš„éªŒè¯ï¼šæ£€æŸ¥å¼•ç”¨æ–‡æœ¬æ˜¯å¦éç©º
            valid_citations += sum(1 for text in anchor_texts if text.strip())
        
        return valid_citations / total_citations if total_citations > 0 else 0.0
    
    @staticmethod
    def calculate_citation_recall(model_answer: List[Dict], reference_answer: str) -> float:
        """è®¡ç®—å¼•ç”¨å¬å›ç‡"""
        # è¿™é‡Œæ˜¯ç®€åŒ–çš„å®ç°ï¼Œå®é™…åº”è¯¥åŸºäºæ›´å¤æ‚çš„è¯­ä¹‰åŒ¹é…
        total_sentences = len(model_answer)
        cited_sentences = sum(1 for sentence in model_answer 
                            if sentence.get('citations', {}).get('anchor_text', []))
        
        return cited_sentences / total_sentences if total_sentences > 0 else 0.0

def main():
    """ä¸»å‡½æ•°"""
    
    # æ ‡é¢˜
    st.markdown('<h1 class="main-header">ğŸ§  å¤§æ¨¡å‹å½’å› åˆ†æå¹³å°</h1>', unsafe_allow_html=True)
    
    # åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
    data_dir = resolve_data_dir(
        os.path.join(BASE_DIR, "data", "BookEvidenceQA_v3"),
        "WEB_LEADERBOARD_DATA_DIR_V3"
    )
    article_dir = resolve_article_dir(
        os.path.join(BASE_DIR, "data", "test_data")
    )
    
    if 'dataset_loader' not in st.session_state:
        with st.spinner("æ­£åœ¨åŠ è½½æ•°æ®é›†..."):
            st.session_state.dataset_loader = DatasetLoader(data_dir)
    
    if 'article_loader' not in st.session_state:
        with st.spinner("æ­£åœ¨åŠ è½½æ–‡ç« æ•°æ®..."):
            st.session_state.article_loader = ArticleLoader(article_dir)
    
    # ä¾§è¾¹æ å¯¼èˆª
    st.sidebar.title("ğŸ“‹ å¯¼èˆªèœå•")
    
    # ä½¿ç”¨å•é€‰æŒ‰é’®æ›¿ä»£ä¸‹æ‹‰èœå•ï¼Œè®©æ‰€æœ‰é€‰é¡¹ç›´æ¥å¯è§
    page = st.sidebar.radio(
        "é€‰æ‹©é¡µé¢",
        ["ğŸ  é¦–é¡µ", "ğŸ” å½’å› å±•ç¤º", "ğŸ“Š æ¨¡å‹è¯„æµ‹", "ğŸ† æ’è¡Œæ¦œ", "ğŸ“š æ•°æ®é›†æµè§ˆ"],
        index=0  # é»˜è®¤é€‰æ‹©é¦–é¡µ
    )
    
    # æ ¹æ®é€‰æ‹©æ˜¾ç¤ºä¸åŒé¡µé¢
    if page == "ğŸ  é¦–é¡µ":
        show_home_page()
    elif page == "ğŸ” å½’å› å±•ç¤º":
        show_attribution_page()
    elif page == "ğŸ“Š æ¨¡å‹è¯„æµ‹":
        show_evaluation_page()
    elif page == "ğŸ† æ’è¡Œæ¦œ":
        show_leaderboard_page()
    elif page == "ğŸ“š æ•°æ®é›†æµè§ˆ":
        show_dataset_page()

def show_home_page():
    """æ˜¾ç¤ºé¦–é¡µ"""
    st.markdown("## ğŸ¯ å¹³å°ç®€ä»‹")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("""
        <div class="metric-card">
        <h3>ğŸ” å½’å› åˆ†æ</h3>
        <p>å±•ç¤ºå¤§æ¨¡å‹å›ç­”çš„å½’å› ä¾æ®ï¼Œæé«˜ç­”æ¡ˆå¯ä¿¡åº¦å’Œå¯è§£é‡Šæ€§</p>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown("""
        <div class="metric-card">
        <h3>ğŸ“Š æ¨¡å‹è¯„æµ‹</h3>
        <p>æ”¯æŒè‡ªå®šä¹‰æ¨¡å‹ä¸Šä¼ è¯„æµ‹ï¼Œè®¡ç®—ç²¾ç¡®çš„å½’å› è´¨é‡æŒ‡æ ‡</p>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        st.markdown("""
        <div class="metric-card">
        <h3>ğŸ† æ•ˆæœå¯¹æ¯”</h3>
        <p>Leaderboardå±•ç¤ºä¸åŒæ¨¡å‹åœ¨å½’å› ä»»åŠ¡ä¸Šçš„è¡¨ç°æ’å</p>
        </div>
        """, unsafe_allow_html=True)
    
    st.markdown("## ğŸ“ˆ æ•°æ®é›†ç»Ÿè®¡")
    
    # æ˜¾ç¤ºæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
    dataset_stats = []
    for domain in st.session_state.dataset_loader.get_domains():
        sample_count = len(st.session_state.dataset_loader.get_sample_ids(domain))
        dataset_stats.append({
            "é¢†åŸŸ": domain.title(),
            # "æ ·æœ¬æ•°é‡": sample_count,
            "æ ·æœ¬æ•°é‡": 200,
            "æ•°æ®ç±»å‹": "é—®ç­”+å½’å› "
        })
    
    if dataset_stats:
        df_stats = pd.DataFrame(dataset_stats)
        st.dataframe(df_stats, width="stretch")
    
    # å¿«é€Ÿå¼€å§‹æŒ‡å—
    st.markdown("## ğŸš€ å¿«é€Ÿå¼€å§‹")
    st.markdown("""
    1. **æµè§ˆæ•°æ®é›†**ï¼šç‚¹å‡»"æ•°æ®é›†æµè§ˆ"æŸ¥çœ‹ä¸åŒé¢†åŸŸçš„é—®ç­”æ ·æœ¬
    2. **æŸ¥çœ‹å½’å› **ï¼šåœ¨"å½’å› å±•ç¤º"é¡µé¢é€‰æ‹©æ ·æœ¬ï¼ŒæŸ¥çœ‹æ¨¡å‹çš„å½’å› åˆ†æ
    3. **è¯„æµ‹æ¨¡å‹**ï¼šåœ¨"æ¨¡å‹è¯„æµ‹"é¡µé¢ä¸Šä¼ æ‚¨çš„æ¨¡å‹è¿›è¡Œè¯„æµ‹
    4. **æŸ¥çœ‹æ’å**ï¼šåœ¨"æ’è¡Œæ¦œ"é¡µé¢æŸ¥çœ‹å„æ¨¡å‹çš„æ€§èƒ½å¯¹æ¯”
    """)

def show_attribution_page():
    """æ˜¾ç¤ºå½’å› å±•ç¤ºé¡µé¢"""
    
    # æ£€æŸ¥æ˜¯å¦åœ¨æŸ¥çœ‹æ–‡ç« çŠ¶æ€
    if st.session_state.get('viewing_article', False):
        show_article_view()
        return
    
    st.markdown("## ğŸ” æ¨¡å‹å½’å› åˆ†æå±•ç¤º")
    
    # é€‰æ‹©æ•°æ®é›†å’Œæ ·æœ¬
    st.markdown("### ğŸ“š é€‰æ‹©æ•°æ®")
    
    domains = st.session_state.dataset_loader.get_domains()
    if not domains:
        st.error("æœªæ‰¾åˆ°å¯ç”¨çš„æ•°æ®é›†")
        return
    
    selected_domain = st.selectbox("é€‰æ‹©é¢†åŸŸ", domains)
    
    sample_ids = st.session_state.dataset_loader.get_sample_ids(selected_domain)
    if not sample_ids:
        st.error(f"é¢†åŸŸ {selected_domain} ä¸­æ²¡æœ‰å¯ç”¨æ ·æœ¬")
        return
    
    # éšæœºé€‰æ‹©æˆ–æ‰‹åŠ¨é€‰æ‹©
    selection_mode = st.radio("é€‰æ‹©æ¨¡å¼", ["éšæœºé€‰æ‹©", "æ‰‹åŠ¨é€‰æ‹©"])
    
    if selection_mode == "éšæœºé€‰æ‹©":
        if st.button("ğŸ² éšæœºé€‰æ‹©ä¸€ä¸ªæ ·æœ¬"):
            st.session_state.selected_sample_id = random.choice(sample_ids)
        
        selected_sample_id = st.session_state.get('selected_sample_id', sample_ids[0])
    else:
        selected_sample_id = st.selectbox("é€‰æ‹©æ ·æœ¬ID", sample_ids)
    
    # é—®ç­”å†…å®¹éƒ¨åˆ†
    st.markdown("### ğŸ’¡ é—®ç­”å†…å®¹")
    
    sample_data = st.session_state.dataset_loader.get_sample(selected_domain, selected_sample_id)
    
    if not sample_data:
        st.error("æ— æ³•åŠ è½½æ ·æœ¬æ•°æ®")
        return
    
    # æ˜¾ç¤ºé—®é¢˜
    st.markdown("**é—®é¢˜:**")
    st.info(sample_data.get('question', ''))
    
    # æ˜¾ç¤ºæ–‡ç« ä¿¡æ¯å’ŒæŒ‰é’®
    source_id = sample_data.get('source', '')
    if source_id:
        st.markdown("### ğŸ“„ åŸæ–‡ç« ")
        
        # è·å–æ–‡ç« ç»Ÿè®¡ä¿¡æ¯
        article_stats = st.session_state.article_loader.get_article_stats(source_id)
        
        if article_stats:
            col1, col2 = st.columns([1, 3])
            
            with col1:
                if st.button("ğŸ“– æŸ¥çœ‹å®Œæ•´æ–‡ç« ", type="primary"):
                    st.session_state.viewing_article = True
                    st.session_state.current_source_id = source_id
                    st.rerun()
            
            with col2:
                # æ˜¾ç¤ºæ–‡ç« ç»Ÿè®¡ä¿¡æ¯
                st.write(f"**æ ‡é¢˜:** {article_stats.get('title', 'æœªçŸ¥')[:80]}...")
                st.write(f"**ä¸»é¢˜:** {article_stats.get('topic', 'æœªçŸ¥')} | **å­—æ•°:** {article_stats.get('word_count', 0):,} | **è¡Œæ•°:** {article_stats.get('line_count', 0):,}")
        else:
            st.warning("æ— æ³•åŠ è½½æ–‡ç« ç»Ÿè®¡ä¿¡æ¯")
    else:
        st.warning("è¯¥æ ·æœ¬ç¼ºå°‘æ–‡ç« æ¥æºä¿¡æ¯")
    
    # æ˜¾ç¤ºå¸¦å½’å› çš„ç­”æ¡ˆ
    st.markdown("**ç­”æ¡ˆ (w/ attribution):**")
    model_answer = sample_data.get('model_answer_rebuild_by_citation', [])
    
    if model_answer:
        # æ„å»ºå¸¦å¼•ç”¨æ ‡æ³¨çš„å®Œæ•´ç­”æ¡ˆ
        full_answer_with_citations = ""
        all_prefix_texts = {}  # å­˜å‚¨æ‰€æœ‰çš„prefix_textï¼Œkeyä¸ºindexèŒƒå›´ï¼Œvalueä¸ºå†…å®¹
        
        for sentence_data in model_answer:
            sentence = sentence_data.get('sentence', '')
            citations = sentence_data.get('citations', {})
            prefix_indices = citations.get('prefix_index', [])
            
            # æ·»åŠ å¥å­å†…å®¹
            full_answer_with_citations += sentence
            
            # æ·»åŠ å¼•ç”¨ç´¢å¼•æ ‡æ³¨
            if prefix_indices:
                citation_tags = ""
                prefix_texts = citations.get('prefix_text', [])
                anchor_texts = citations.get('anchor_text', [])
                
                # ç¡®ä¿ç´¢å¼•ã€å‰ç¼€æ–‡æœ¬å’Œé”šç‚¹æ–‡æœ¬æ•°é‡ä¸€è‡´
                for i, prefix_idx in enumerate(prefix_indices):
                    if isinstance(prefix_idx, list) and len(prefix_idx) >= 2:
                        start_idx, end_idx = prefix_idx[0], prefix_idx[-1]
                        citation_tags += f"{{{start_idx}-{end_idx}}}"
                        
                        # æ­£ç¡®åŒ¹é…å¯¹åº”çš„prefix_textå’Œanchor_text
                        if i < len(prefix_texts) and i < len(anchor_texts):
                            all_prefix_texts[f"{start_idx}-{end_idx}"] = {
                                'prefix_text': prefix_texts[i],
                                'anchor_text': anchor_texts[i]
                            }
                
                if citation_tags:
                    # ä¸ºå¼•ç”¨æ ‡æ³¨æ·»åŠ æ ·å¼
                    styled_tags = f'<span style="background-color: #e1f5fe; color: #0277bd; font-size: 0.85em; padding: 2px 4px; border-radius: 3px; font-weight: bold;">{citation_tags}</span>'
                    full_answer_with_citations += styled_tags
            
            full_answer_with_citations += " "
        
        # æ˜¾ç¤ºå®Œæ•´çš„å¸¦å¼•ç”¨æ ‡æ³¨çš„ç­”æ¡ˆ
        st.markdown(f'<div style="padding: 0.75rem; background-color: #d4edda; border: 1px solid #c3e6cb; border-radius: 0.25rem; color: #155724;">{full_answer_with_citations.strip()}</div>', unsafe_allow_html=True)
        
        # æ˜¾ç¤ºå¼•ç”¨æ–‡æœ¬è¯¦æƒ…ï¼ˆå¯æŠ˜å ï¼‰
        if all_prefix_texts:
            with st.expander("ğŸ“š æŸ¥çœ‹å¼•ç”¨æ–‡æœ¬è¯¦æƒ…", expanded=True):
                for idx_range, citation_data in all_prefix_texts.items():
                    styled_index = f'<span style="background-color: #e1f5fe; color: #0277bd; font-size: 0.85em; padding: 2px 4px; border-radius: 3px; font-weight: bold;">{{{idx_range}}}</span>'
                    st.markdown(f"**å¼•ç”¨ {styled_index}:**", unsafe_allow_html=True)
                    
                    prefix_text = citation_data['prefix_text']
                    anchor_text = citation_data['anchor_text']
                    
                    # åœ¨prefix_textä¸­é«˜äº®æ˜¾ç¤ºanchor_text
                    if anchor_text and anchor_text.strip():
                        # å°†anchor_textåŠ ç²—å¹¶æ·»åŠ èƒŒæ™¯è‰²
                        highlighted_text = prefix_text.replace(
                            anchor_text, 
                            f"<mark><strong>{anchor_text}</strong></mark>"
                        )
                        st.markdown(highlighted_text, unsafe_allow_html=True)
                    else:
                        st.write(prefix_text)
                    
                    st.markdown("---")  # åˆ†éš”çº¿
    else:
        st.warning("è¯¥æ ·æœ¬æš‚æ— å¸¦å½’å› çš„ç­”æ¡ˆæ•°æ®")
    
    # æ˜¾ç¤ºå‚è€ƒç­”æ¡ˆï¼ˆanswer_w/o_attributionï¼‰
    st.markdown("**å‚è€ƒç­”æ¡ˆ (w/o attribution):**")
    st.warning(sample_data.get('answer', ''))

def show_article_view():
    """æ˜¾ç¤ºæ–‡ç« å…¨æ–‡é¡µé¢"""
    source_id = st.session_state.get('current_source_id', '')
    
    # è¿”å›æŒ‰é’®
    col1, col2 = st.columns([1, 4])
    with col1:
        if st.button("â† è¿”å›å½’å› å±•ç¤º", type="secondary"):
            st.session_state.viewing_article = False
            st.session_state.current_source_id = ""
            st.rerun()
    
    with col2:
        st.markdown("## ğŸ“– æ–‡ç« å…¨æ–‡")
    
    if not source_id:
        st.error("æœªæ‰¾åˆ°æ–‡ç« ID")
        return
    
    # è·å–æ–‡ç« å†…å®¹å’Œç»Ÿè®¡ä¿¡æ¯
    article_content = st.session_state.article_loader.get_article(source_id)
    article_stats = st.session_state.article_loader.get_article_stats(source_id)
    
    if not article_content:
        st.error("æœªæ‰¾åˆ°å¯¹åº”çš„æ–‡ç« å†…å®¹")
        return
    
    # æ˜¾ç¤ºæ–‡ç« ç»Ÿè®¡ä¿¡æ¯
    if article_stats:
        st.markdown("### ğŸ“Š æ–‡ç« ä¿¡æ¯")
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("ä¸»é¢˜", article_stats.get('topic', 'æœªçŸ¥'))
        with col2:
            st.metric("å­—æ•°", f"{article_stats.get('word_count', 0):,}")
        with col3:
            st.metric("å­—ç¬¦æ•°", f"{article_stats.get('char_count', 0):,}")
        with col4:
            st.metric("è¡Œæ•°", f"{article_stats.get('line_count', 0):,}")
        
        st.markdown(f"**æ ‡é¢˜:** {article_stats.get('title', 'æœªçŸ¥')}")
        st.markdown("---")
    
    # æ˜¾ç¤ºæ–‡ç« å†…å®¹
    st.markdown("### ğŸ“„ æ–‡ç« å†…å®¹")
    
    # å°†æ–‡ç« å†…å®¹åˆ†æ®µæ˜¾ç¤ºï¼Œæé«˜å¯è¯»æ€§
    lines = article_content.split('\n')
    current_section = ""
    
    for line in lines:
        line = line.strip()
        if not line:
            if current_section:
                st.markdown(current_section)
                current_section = ""
            continue
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ ‡é¢˜æˆ–ç« èŠ‚
        if (line.startswith('#') or 
            ('**' in line and len(line) < 100) or 
            (line.isupper() and len(line) < 100)):
            
            if current_section:
                st.markdown(current_section)
                current_section = ""
            
            # æ˜¾ç¤ºæ ‡é¢˜
            if line.startswith('#'):
                st.markdown(line)
            else:
                st.markdown(f"### {line.replace('**', '')}")
        else:
            current_section += line + " "
            
            # æ¯æ®µçº¦500å­—ç¬¦å°±æ˜¾ç¤ºä¸€æ¬¡ï¼Œé¿å…å•æ®µè¿‡é•¿
            if len(current_section) > 500:
                st.markdown(current_section)
                current_section = ""
    
    # æ˜¾ç¤ºå‰©ä½™å†…å®¹
    if current_section:
        st.markdown(current_section)
    

def show_evaluation_page():
    """æ˜¾ç¤ºæ¨¡å‹è¯„æµ‹é¡µé¢"""
    st.markdown("## ğŸ“Š Trust-Score æ¨¡å‹è¯„æµ‹")
    
    if TrustScoreManager is None:
        st.error("Trust-Scoreç®¡ç†å™¨æœªèƒ½åŠ è½½ï¼Œè¯·æ£€æŸ¥ä¾èµ–æ¨¡å—")
        return
    
    st.markdown("""
    ### ğŸ¯ Trust-Score è¯„æµ‹è¯´æ˜
    Trust-Scoreæ˜¯ä¸€ä¸ªç»¼åˆè¯„ä¼°å¤§æ¨¡å‹åœ¨RAGä»»åŠ¡ä¸­å¯ä¿¡åº¦çš„æŒ‡æ ‡ï¼ŒåŒ…å«ï¼š
    - **å“åº”æ­£ç¡®æ€§**: è¯„ä¼°ç”Ÿæˆå›ç­”çš„å‡†ç¡®æ€§
    - **å¼•ç”¨è´¨é‡**: è¯„ä¼°å¼•ç”¨æ ‡æ³¨çš„è´¨é‡ï¼ˆå¬å›ç‡å’Œç²¾ç¡®ç‡ï¼‰
    - **æ‹’ç­”åˆç†æ€§**: è¯„ä¼°æ¨¡å‹åœ¨ä¿¡æ¯ä¸è¶³æ—¶çš„æ‹’ç­”èƒ½åŠ›
    """)
    
    # åˆå§‹åŒ–Trust-Scoreç®¡ç†å™¨
    if 'trust_manager' not in st.session_state:
        st.session_state.trust_manager = TrustScoreManager()
    
    # æ¨¡å‹é…ç½®è¡¨å•
    with st.form("trust_score_config"):
        st.markdown("### ğŸ”§ æ¨¡å‹é…ç½®")
        col1, col2 = st.columns(2)
        
        with col1:
            model_name = st.text_input(
                "æ¨¡å‹åç§°", 
                placeholder="ä¾‹å¦‚: gpt-4o-mini, claude-3-sonnet",
                help="è¾“å…¥æ‚¨è¦è¯„æµ‹çš„æ¨¡å‹åç§°"
            )
            api_key = st.text_input(
                "API Key", 
                type="password",
                help="è¾“å…¥æ¨¡å‹å¯¹åº”çš„APIå¯†é’¥"
            )
        
        with col2:
            dataset_type = st.selectbox(
                "è¯„æµ‹æ•°æ®é›†", 
                ["BookEvidenceQA_v3", "alce"],
                help="BookEvidenceQA_v3: BookEvidenceQA 20ä¸ªé¢†åŸŸ | alce: ASQA/ELI5/QAMPARI 3ä¸ªæ•°æ®é›†"
            )
            max_samples = st.number_input(
                "æœ€å¤§æ ·æœ¬æ•°é‡", 
                min_value=1, 
                max_value=1000, 
                value=10,
                help="é™åˆ¶æ¯ä¸ªé¢†åŸŸ/æ•°æ®é›†çš„è¯„æµ‹æ ·æœ¬æ•°é‡ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰"
            )
        
        # é«˜çº§é…ç½®
        st.markdown("### âš™ï¸ é«˜çº§é…ç½®")
        col1, col2 = st.columns(2)
        
        with col1:
            use_autoais = st.checkbox(
                "ä½¿ç”¨AutoAISæ¨¡å‹", 
                value=True,
                help="å¯ç”¨AutoAISè¿›è¡Œå¼•ç”¨è´¨é‡è¯„ä¼°ï¼Œç¦ç”¨åˆ™ä½¿ç”¨ç®€å•åŒ¹é…ï¼ˆé€‚ç”¨äºæ— GPUç¯å¢ƒï¼‰"
            )
        
        with col2:
            rejection_flag = st.text_input(
                "æ‹’ç­”æ ‡è¯†", 
                value="æˆ‘æ— æ³•æ‰¾åˆ°ç­”æ¡ˆ",
                help="æ¨¡å‹æ‹’ç­”æ—¶ä½¿ç”¨çš„æ ‡è¯†æ–‡æœ¬"
            )
        
        submitted = st.form_submit_button("ğŸš€ å¼€å§‹Trust-Scoreè¯„æµ‹", type="primary")
    
    if submitted:
        if not model_name or not api_key:
            st.error("è¯·å¡«å†™æ¨¡å‹åç§°å’ŒAPI Key")
            return
        
        # å¼€å§‹Trust-Scoreè¯„æµ‹
        st.markdown("### ğŸ“ˆ Trust-Scoreè¯„æµ‹è¿›è¡Œä¸­...")
        
        # æ˜¾ç¤ºè¯„æµ‹ä¿¡æ¯
        info_container = st.container()
        with info_container:
            col1, col2, col3 = st.columns(3)
            with col1:
                st.info(f"**æ¨¡å‹**: {model_name}")
            with col2:
                st.info(f"**æ•°æ®é›†**: {dataset_type.upper()}")
            with col3:
                st.info(f"**æ ·æœ¬æ•°**: {max_samples}")
        
        # è¿›åº¦æ¡å’ŒçŠ¶æ€
        progress_bar = st.progress(0)
        status_text = st.empty()
        log_container = st.expander("ğŸ“‹ è¯¦ç»†æ—¥å¿—", expanded=False)
        
        try:
            # è¿è¡Œè¯„æµ‹
            with st.spinner("æ­£åœ¨è¿è¡ŒTrust-Scoreè¯„æµ‹..."):
                status_text.text("æ­£åœ¨å¯åŠ¨è¯„æµ‹è¿›ç¨‹...")
                progress_bar.progress(0.1)
                
                # è°ƒç”¨trust_score_managerè¿è¡Œè¯„æµ‹
                success, message = st.session_state.trust_manager.run_evaluation(
                    model_name=model_name,
                    api_key=api_key,
                    dataset_type=dataset_type,
                    max_samples=max_samples,
                    use_autoais=use_autoais
                )
                
                progress_bar.progress(0.5)
                status_text.text("è¯„æµ‹è¿›ç¨‹è¿è¡Œä¸­...")
                
                if success:
                    progress_bar.progress(1.0)
                    status_text.text("è¯„æµ‹å®Œæˆï¼æ­£åœ¨è¯»å–ç»“æœ...")
                    
                    # è¯»å–æœ€æ–°ç»“æœ
                    all_results = st.session_state.trust_manager.get_latest_results(dataset_type)
                    
                    # æ‰¾åˆ°å½“å‰æ¨¡å‹çš„æœ€æ–°ç»“æœ
                    latest_results = None
                    for result in all_results:
                        if result['model_name'] == model_name:
                            latest_results = result['data']
                            break
                    
                    if latest_results:
                        st.success("ğŸ‰ Trust-Scoreè¯„æµ‹å®Œæˆï¼")
                        
                        # æ˜¾ç¤ºè¯„æµ‹ç»“æœ
                        st.markdown("### ğŸ“Š è¯„æµ‹ç»“æœ")
                        
                        # æå–å…³é”®æŒ‡æ ‡
                        summary = latest_results.get("evaluation_info", {})
                        detailed = latest_results.get("summary", {}).get("domain_rankings", [])
                        
                        if detailed:
                            # è®¡ç®—å¹³å‡æŒ‡æ ‡
                            avg_metrics = {
                                "reject_f1": sum(d.get("reject_score", 0) for d in detailed) / len(detailed),
                                "answered_str_em": sum(d.get("answered_str_em", 0) for d in detailed) / len(detailed), 
                                "answered_citation_f1": sum(d.get("answered_citation_f1", 0) for d in detailed) / len(detailed),
                                "trust_score": summary.get("average_trust_score", 0)
                            }
                            
                            # æ˜¾ç¤ºæŒ‡æ ‡å¡ç‰‡
                            col1, col2, col3, col4 = st.columns(4)
                            
                            with col1:
                                st.metric("æ‹’ç­”F1", f"{avg_metrics['reject_f1']:.3f}")
                            with col2:
                                st.metric("å›ç­”å‡†ç¡®æ€§", f"{avg_metrics['answered_str_em']:.3f}")
                            with col3:
                                st.metric("å¼•ç”¨è´¨é‡F1", f"{avg_metrics['answered_citation_f1']:.3f}")
                            with col4:
                                st.metric("**Trust-Score**", f"{avg_metrics['trust_score']:.3f}")
                            
                            # ç»“æœå¯è§†åŒ–
                            fig = px.bar(
                                x=list(avg_metrics.keys()),
                                y=list(avg_metrics.values()),
                                title=f"{model_name} åœ¨ {dataset_type.upper()} æ•°æ®é›†ä¸Šçš„Trust-Scoreç»“æœ",
                                labels={'x': 'æŒ‡æ ‡', 'y': 'åˆ†æ•°'},
                                color=list(avg_metrics.values()),
                                color_continuous_scale="viridis"
                            )
                            fig.update_layout(showlegend=False)
                            st.plotly_chart(fig, width="stretch")
                            
                            # è¯¦ç»†ç»“æœå±•ç¤º
                            with st.expander("ğŸ“‹ è¯¦ç»†ç»“æœ", expanded=False):
                                if dataset_type == "BookEvidenceQA_v3":
                                    st.markdown("#### å„é¢†åŸŸè¯¦ç»†ç»“æœ")
                                else:
                                    st.markdown("#### å„æ•°æ®é›†è¯¦ç»†ç»“æœ")
                                
                                detail_df = pd.DataFrame(detailed)
                                if not detail_df.empty:
                                    st.dataframe(detail_df, width="stretch")
                        
                        st.success(f"âœ… ç»“æœå·²è‡ªåŠ¨ä¿å­˜åˆ°æ’è¡Œæ¦œï¼\n\n{message}")
                        
                    else:
                        st.error("æœªèƒ½è¯»å–è¯„æµ‹ç»“æœï¼Œè¯·æ£€æŸ¥è¯„æµ‹æ˜¯å¦æˆåŠŸå®Œæˆã€‚")
                else:
                    progress_bar.progress(0.0)
                    status_text.text("è¯„æµ‹å¤±è´¥")
                    st.error(f"âŒ è¯„æµ‹å¤±è´¥: {message}")
                    
                    with log_container:
                        st.text(message)
                        
        except Exception as e:
            st.error(f"âŒ è¯„æµ‹è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            progress_bar.progress(0.0)
            status_text.text("è¯„æµ‹å¼‚å¸¸ç»ˆæ­¢")
            
            with log_container:
                st.text(f"é”™è¯¯è¯¦æƒ…: {str(e)}")

def show_leaderboard_page():
    """æ˜¾ç¤ºæ’è¡Œæ¦œé¡µé¢"""
    st.markdown("## ğŸ† Trust-Score æ’è¡Œæ¦œ")
    
    if TrustScoreManager is None:
        st.error("Trust-Scoreç®¡ç†å™¨æœªèƒ½åŠ è½½ï¼Œè¯·æ£€æŸ¥ä¾èµ–æ¨¡å—")
        return
    
    # åˆå§‹åŒ–Trust-Scoreç®¡ç†å™¨
    if 'trust_manager' not in st.session_state:
        st.session_state.trust_manager = TrustScoreManager()
    
    # æ•°æ®é›†é€‰æ‹©
    dataset_tab = st.selectbox(
        "é€‰æ‹©æ’è¡Œæ¦œ",
        ["BookEvidenceQA_v3", "ALCEæ•°æ®é›†"],
        help="é€‰æ‹©è¦æŸ¥çœ‹çš„æ’è¡Œæ¦œç±»å‹"
    )
    
    dataset_type = "BookEvidenceQA_v3" if "BookEvidenceQA" in dataset_tab else "alce"
    
    try:
        # è¯»å–æ’è¡Œæ¦œæ•°æ®
        leaderboard_data = st.session_state.trust_manager.get_leaderboard(dataset_type)
        
        if not leaderboard_data:
            st.warning(f"æš‚æ—  {dataset_tab} çš„è¯„æµ‹æ•°æ®ã€‚è¯·å…ˆåœ¨ã€Œæ¨¡å‹è¯„æµ‹ã€é¡µé¢è¿è¡Œè¯„æµ‹ã€‚")
            
            # æ˜¾ç¤ºç¤ºä¾‹è¯´æ˜
            st.markdown("### ğŸ“‹ æ’è¡Œæ¦œè¯´æ˜")
            st.markdown("""
            **Trust-Scoreæ’è¡Œæ¦œæŒ‡æ ‡è¯´æ˜**ï¼š
            - **reject_f1**: æ‹’ç­”F1åˆ†æ•°ï¼Œè¡¡é‡æ¨¡å‹åœ¨ä¿¡æ¯ä¸è¶³æ—¶æ­£ç¡®æ‹’ç­”çš„èƒ½åŠ›
            - **answered_str_em**: å›ç­”å‡†ç¡®æ€§ï¼Œä½¿ç”¨GPT-4o-miniè¯„ä¼°è¯­ä¹‰ç›¸ä¼¼æ€§
            - **answered_citation_f1**: å¼•ç”¨è´¨é‡F1ï¼Œä½¿ç”¨AutoAISæ¨¡å‹è¯„ä¼°å¼•ç”¨æ”¯æŒåº¦
            - **trust_score**: ç»¼åˆå¯ä¿¡åº¦åˆ†æ•°ï¼Œç»“åˆä»¥ä¸Šä¸‰ä¸ªç»´åº¦
            """)
            return
        
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(leaderboard_data)
        df = df.sort_values("trust_score", ascending=False)
        df.reset_index(drop=True, inplace=True)
        df.index = df.index + 1
        
        # æ·»åŠ æ’ååˆ—
        df.insert(0, "æ’å", df.index)
        
        # æ ¼å¼åŒ–æ•°å€¼åˆ—
        numeric_cols = ["reject_f1", "answered_str_em", "answered_citation_f1", "trust_score"]
        for col in numeric_cols:
            if col in df.columns:
                df[col] = df[col].round(2)
        
        # æ˜¾ç¤ºæ’è¡Œæ¦œæ ‡é¢˜
        st.markdown(f"### ğŸ“Š {dataset_tab} æ’è¡Œæ¦œ")
        
        # æ˜¾ç¤ºæ’è¡Œæ¦œè¡¨æ ¼
        st.dataframe(
            df.style.highlight_max(axis=0, subset=numeric_cols),
            width="stretch"
        )
        
        # ç»Ÿè®¡ä¿¡æ¯
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("å‚ä¸æ¨¡å‹æ•°", len(df))
        with col2:
            if len(df) > 0:
                best_model = df.iloc[0]["model_name"]
                best_score = df.iloc[0]["trust_score"]
                st.metric("æœ€ä½³æ¨¡å‹", best_model)
            else:
                st.metric("æœ€ä½³æ¨¡å‹", "æš‚æ— ")
        with col3:
            if len(df) > 0:
                # ä½¿ç”¨åŸå§‹æ•°æ®ä¸­çš„average_trust_scoreè€Œä¸æ˜¯è®¡ç®—å¹³å‡å€¼
                dataset_avg_score = st.session_state.trust_manager.get_dataset_average_trust_score(dataset_type)
                st.metric("å¹³å‡Trust-Score", f"{dataset_avg_score:.3f}")
            else:
                st.metric("å¹³å‡Trust-Score", "0.000")
        
        # å¯è§†åŒ–å¯¹æ¯”
        if len(df) > 0:
            st.markdown("### ğŸ“ˆ æ€§èƒ½å¯¹æ¯”å›¾")
            
            col1, col2 = st.columns(2)
            
            with col1:
                # Trust-Scoreå¯¹æ¯”
                fig1 = px.bar(
                    df, 
                    x="model_name", 
                    y="trust_score",
                    title="Trust-Scoreå¯¹æ¯”",
                    color="trust_score",
                    color_continuous_scale="viridis",
                    labels={"model_name": "æ¨¡å‹åç§°", "trust_score": "Trust-Score"}
                )
                fig1.update_layout(showlegend=False)
                st.plotly_chart(fig1, width="stretch")
            
            with col2:
                # å›ç­”å‡†ç¡®æ€§å¯¹æ¯”  
                fig2 = px.bar(
                    df,
                    x="model_name",
                    y="answered_str_em", 
                    title="å›ç­”å‡†ç¡®æ€§å¯¹æ¯”",
                    color="answered_str_em",
                    color_continuous_scale="plasma",
                    labels={"model_name": "æ¨¡å‹åç§°", "answered_str_em": "å›ç­”å‡†ç¡®æ€§"}
                )
                fig2.update_layout(showlegend=False)
                st.plotly_chart(fig2, width="stretch")
            
            # é›·è¾¾å›¾å¯¹æ¯”
            st.markdown("### ğŸ¯ ç»¼åˆèƒ½åŠ›é›·è¾¾å›¾")
            
            selected_models = st.multiselect(
                "é€‰æ‹©è¦å¯¹æ¯”çš„æ¨¡å‹", 
                df["model_name"].tolist(),
                default=df["model_name"].tolist()[:3] if len(df) >= 3 else df["model_name"].tolist()
            )
            
            if selected_models:
                fig = go.Figure()
                
                metrics = ["reject_f1", "answered_str_em", "answered_citation_f1", "trust_score"]
                metric_labels = ["æ‹’ç­”F1", "å›ç­”å‡†ç¡®æ€§", "å¼•ç”¨è´¨é‡F1", "Trust-Score"]
                
                for model in selected_models:
                    model_data = df[df["model_name"] == model].iloc[0]
                    values = [model_data[metric] for metric in metrics]
                    values.append(values[0])  # é—­åˆé›·è¾¾å›¾
                    
                    fig.add_trace(go.Scatterpolar(
                        r=values,
                        theta=metric_labels + [metric_labels[0]],
                        fill='toself',
                        name=model
                    ))
                
                fig.update_layout(
                    polar=dict(
                        radialaxis=dict(
                            visible=True,
                            range=[0, 1]
                        )),
                    showlegend=True,
                    title="æ¨¡å‹ç»¼åˆèƒ½åŠ›å¯¹æ¯”"
                )
                
                st.plotly_chart(fig, width="stretch")
    
    except Exception as e:
        st.error(f"åŠ è½½æ’è¡Œæ¦œæ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        st.info("è¯·æ£€æŸ¥trust_score_resultsç›®å½•æ˜¯å¦å­˜åœ¨è¯„æµ‹ç»“æœæ–‡ä»¶ã€‚")

def show_dataset_page():
    """æ˜¾ç¤ºæ•°æ®é›†æµè§ˆé¡µé¢"""
    st.markdown("## ğŸ“š æ•°æ®é›†æµè§ˆä¸ç»Ÿè®¡")
    
    domains = st.session_state.dataset_loader.get_domains()
    
    if not domains:
        st.error("æœªæ‰¾åˆ°å¯ç”¨çš„æ•°æ®é›†")
        return
    
    # æ•°æ®é›†æ¦‚è§ˆ
    st.markdown("### ğŸ“Š æ•°æ®é›†æ¦‚è§ˆ")
    
    overview_data = []
    for domain in domains:
        sample_ids = st.session_state.dataset_loader.get_sample_ids(domain)
        sample_count = len(sample_ids)
        
        # éšæœºæŠ½æ ·åˆ†æ
        if sample_count > 0:
            sample_data = st.session_state.dataset_loader.get_sample(domain, sample_ids[0])
            avg_question_len = len(sample_data.get('question', ''))
            avg_answer_len = len(sample_data.get('answer', ''))
            citation_count = len(sample_data.get('model_answer_rebuild_by_citation', []))
        else:
            avg_question_len = avg_answer_len = citation_count = 0
        
        overview_data.append({
            "é¢†åŸŸ": domain.title(),
            # "æ ·æœ¬æ•°é‡": sample_count,
            "æ ·æœ¬æ•°é‡": 200,
            "å¹³å‡é—®é¢˜é•¿åº¦": avg_question_len,
            "å¹³å‡ç­”æ¡ˆé•¿åº¦": avg_answer_len,
            "å¹³å‡å¼•ç”¨å¥æ•°": citation_count
        })
    
    df_overview = pd.DataFrame(overview_data)
    st.dataframe(df_overview, width="stretch")
    
    # è¯¦ç»†æµè§ˆ
    st.markdown("### ğŸ” è¯¦ç»†æ•°æ®æµè§ˆ")
    
    selected_domain = st.selectbox("é€‰æ‹©è¦æµè§ˆçš„é¢†åŸŸ", domains)
    sample_ids = st.session_state.dataset_loader.get_sample_ids(selected_domain)
    
    # åˆ†é¡µæ˜¾ç¤º
    page_size = 5
    total_pages = (len(sample_ids) + page_size - 1) // page_size
    
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        current_page = st.selectbox(
            f"é¡µç  (å…± {total_pages} é¡µ)", 
            range(1, total_pages + 1)
        )
    
    start_idx = (current_page - 1) * page_size
    end_idx = min(start_idx + page_size, len(sample_ids))
    current_samples = sample_ids[start_idx:end_idx]
    
    # æ˜¾ç¤ºå½“å‰é¡µçš„æ ·æœ¬
    for i, sample_id in enumerate(current_samples):
        sample_data = st.session_state.dataset_loader.get_sample(selected_domain, sample_id)
        
        with st.expander(f"æ ·æœ¬ {start_idx + i + 1}: {sample_id}", expanded=False):
            col1, col2 = st.columns([1, 1])
            
            with col1:
                st.markdown("**é—®é¢˜:**")
                st.write(sample_data.get('question', ''))
                
                st.markdown("**å‚è€ƒç­”æ¡ˆ:**")
                st.write(sample_data.get('answer', ''))
            
            with col2:
                model_answer = sample_data.get('model_answer_rebuild_by_citation', [])
                st.markdown(f"**å½’å› å¥æ•°:** {len(model_answer)}")
                
                if model_answer:
                    total_citations = sum(
                        len(sentence.get('citations', {}).get('anchor_text', []))
                        for sentence in model_answer
                    )
                    st.markdown(f"**æ€»å¼•ç”¨æ•°:** {total_citations}")
                    
                    # æ˜¾ç¤ºç¬¬ä¸€ä¸ªå¥å­ä½œä¸ºç¤ºä¾‹
                    first_sentence = model_answer[0].get('sentence', '')
                    st.markdown("**é¦–å¥ç¤ºä¾‹:**")
                    st.write(first_sentence[:100] + "..." if len(first_sentence) > 100 else first_sentence)

if __name__ == "__main__":
    main()
